"""
Target Problem:
---------------
* A classifier for the diagnosis of Autism Spectrum Disorder (ASD)

Proposed Solution (Machine Learning Pipeline):
----------------------------------------------
* Constant Feature Elimination -> SelectKBest Algorithm -> Gradient Boosting

Input to Proposed Solution:
---------------------------
* Directories of training and testing data in csv file format
* These two types of data should be stored in n x m pattern in csv file format.

  Typical Example:
  ----------------
  n x m samples in training csv file (n number of samples, m - 1 number of features, ground truth labels at last column)
  k x s samples in testing csv file (k number of samples, s number of features)

* These data set files are ready by load_data() function.
* For comprehensive information about input format, please check the section
  "Data Sets and Usage Format of Source Codes" in README.md file on github.

Output of Proposed Solution:
----------------------------
* Predictions generated by learning model for testing set
* They are stored in "submission.csv" file.

Code Owner:
-----------
* Copyright © Team 1. All rights reserved.
* Copyright © Istanbul Technical University, Learning From Data Spring 2019. All rights reserved. """


import csv
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.feature_selection import chi2, SelectKBest


def load_data(filename):
    train_data = pd.read_csv(filename[0])
    test_data = pd.read_csv(filename[1])
    return train_data, test_data


def filter_feature_selection(X, y, nof_features):

    """
    This function performs selectKBest filtering algorithm to compute importance scores of features.
    Then, these scores are concatenated with features themselves.
    Finally, top k most important features are selected.

    Parameters
    ----------
    X: features of train data set
    y: labels of train data set
    nof_features: number of features to be selected """

    best_features = SelectKBest(score_func=chi2, k="all")
    best_features.fit(X, y)
    df_scores = pd.DataFrame(best_features.scores_)
    df_columns = pd.DataFrame(X.columns)

    # Concatenate features and related score for visualization
    feature_scores = pd.concat([df_columns, df_scores], axis=1)
    feature_scores.columns = ['Specs', 'Score']

    # Select top K features
    ft = feature_scores.nlargest(nof_features, 'Score')
    features = ft.index.values
    return features


def preprocessing(trainData):

    """
    The method splits the train dataset into features and labels, which are X and y.
    Then, it drops constant features, and chooses top 341 features.

    Parameters
    ----------
    trainData: It is numpy array containing features and labels together
    :return: It returns train dataset with selected features, labels, and index values of chosen features
    """

    X = trainData.iloc[:, 0:595]
    y = trainData.iloc[:, -1]

    # Drop zero columns and Select top K features with Filter Method
    X = X.drop(["X3", "X31", "X32", "X127", "X128", "X590"], axis=1)
    feature_indexes = filter_feature_selection(X, y, 341)

    return X.values[:, feature_indexes], y, feature_indexes


def train_model(X_train, y_train):

    """
    A learning model is trained by using train data set.
    Gradient Boosting classifier is preferred.

    Parameters
    ----------
    X_train: train dataset with top k most important features
    y_train: labels of those samples
    """

    gradBoost = GradientBoostingClassifier(
        n_estimators=6,
        learning_rate=1,
        max_features=2,
        max_depth=2,
        random_state=289)

    gradBoost.fit(X_train, y_train)
    return gradBoost


def transform_test_samples(test_data, selected_features):

    """
    This method applies preprocessing operations to test data.
    It at first eliminates constant features, and then choose top k most relevant features from the data set.

    Parameters
    ----------
    test_data: test data set
    selected_features: index values of selected features
    """
    X = test_data.iloc[:, 0:595]
    X = X.drop(["X3", "X31", "X32", "X127", "X128", "X590"], axis=1)
    return X.values[:, selected_features]


def predict(model, X_test):

    """
    This method predicts outputs for test samples by using learning model.

    Parameters
    ----------
    model: learning model for prediction
    X_test: testing dataset
    :return: predictions for testing set
    """
    return model.predict(X_test)


def write_output(predictions):

    submissionFile = [["ID", "Predicted"]]
    for i, prediction in enumerate(predictions):
        submissionFile.append([i + 1, int(prediction)])

    # Write to file
    with open('Submission.csv', 'w') as csvFile:
        writer = csv.writer(csvFile)
        writer.writerows(submissionFile)


# ******* Main Program ******* #

Xpaths = ["train.csv", "test.csv"]
trainData, testData = load_data(Xpaths)

X_train, y_train, selected_features = preprocessing(trainData)
gradientBoost = train_model(X_train, y_train)

XtestNew = transform_test_samples(testData, selected_features)
predictions = predict(gradientBoost, XtestNew)
write_output(predictions)
