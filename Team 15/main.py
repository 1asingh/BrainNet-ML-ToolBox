"""
Target Problem:
---------------
* A classifier for the diagnosis of Autism Spectrum Disorder (ASD)

Proposed Solution (Machine Learning Pipeline):
----------------------------------------------
* Isolation Forest -> RFECV Algorithm -> Bagging Classifier (Base: SVM)

Input to Proposed Solution:
---------------------------
* Directories of training and testing data in csv file format
* These two types of data should be stored in n x m pattern in csv file format.

  Typical Example:
  ----------------
  n x m samples in training csv file (n number of samples, m - 1 number of features, ground truth labels at last column)
  k x s samples in testing csv file (k number of samples, s number of features)

* These data set files are ready by load_data() function.
* For comprehensive information about input format, please check the section
  "Data Sets and Usage Format of Source Codes" in README.md file on github.

Output of Proposed Solution:
----------------------------
* Predictions generated by learning model for testing set
* They are stored in "submission.csv" file.

Code Owner:
-----------
* Copyright © Team 15. All rights reserved.
* Copyright © Istanbul Technical University, Learning From Data Spring 2019. All rights reserved. """

import csv
import numpy as np
from statistics import mean
from sklearn.ensemble import BaggingClassifier, IsolationForest

from sklearn.svm import SVC
from sklearn.model_selection import ShuffleSplit, GridSearchCV
from sklearn.feature_selection import RFECV

np.random.seed(7)


def load_data():

    """
    * Train data is read from "train.csv" file.
    * The whole content which is read from the file is decomposed into sub parts called "row".
    * Each sub part is collected in the list "contents".
    * Then, this list is converted into numpy array and split into features and labels of training samples.

    * Test data is read from "test.csv" file.
    * The whole content which is read from the file is decomposed into sub parts called "row".
    * Each sub part is collected in the list "contents".
    * Then, this list is converted into numpy array called "x_test".

    """

    contents = []
    with open('train.csv') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',',)
        next(csv_reader)
        for row in csv_reader:
                contents += [row]

    cont_np = np.asarray(contents, dtype=np.float64)
    train_x = cont_np[:, :-1]
    train_y = cont_np[:, -1]

    contents = []
    with open('test.csv') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',',)
        next(csv_reader)
        for row in csv_reader:
                contents += [row]

    test_x = np.asarray(contents, dtype=np.float64)

    return train_x, train_y, test_x


def outlier_detection(train_x, train_y):

    """
    * The outliers are the samples which do not fit general data distribution trend.
      Those samples mislead the learning model during training phase.
      Hence, eliminating outliers is beneficial process.

    * This method detects outliers by using Isolation Forest model.
    * Then, it discards those samples from training data.

    Parameters
    ----------
    train_x: features of training data
    train_y: labels of training data
    """

    clf = IsolationForest(behaviour='new', random_state=1, contamination='auto')
    preds = clf.fit_predict(train_x)
    for i in range(0, len(preds)):
        if preds[i] == -1:
            train_x = np.delete(train_x, i, 0)
            train_y = np.delete(train_y, i, 0)

    return train_x, train_y


def feature_selection(train_x, train_y, test_x):

    """
    The method uses Recursive Feature Elimination Feature method to choose subset of features.
    It is a wrapper method of feature selection techniques.
    The main purpose is to reduce the dimension of the samples to avoid curse of dimensionality.

    Parameters
    ----------
    train_x: features of training data
    test_x: features of testing data
    """

    svc = SVC(kernel="linear")
    rfecv = RFECV(estimator=svc, step=1, cv=ShuffleSplit(n_splits=10, test_size=0.25, random_state=0),
                  n_jobs=-1, scoring='accuracy')

    reduced_train_x = rfecv.fit_transform(train_x, train_y)
    reduced_test_x = rfecv.transform(test_x)
    return reduced_train_x, reduced_test_x


def svc_param_selection(X, y, cv):

    """
    The method aims to find best parameters for svc learning model.
    To accomplish this, It uses Grid Search Cross Validation method.

    A set of values are determined for each parameter of svc learning model.
    Grid Search chooses one of those values which maximizes the classification accuracy.

    Parameters
    ----------
    X: features of training data
    y: labels of training data
    cv: cross validation object which will be applied during parameter search operation
    """

    # a set of values for svc parameters
    param_c = [10**i for i in range(-11, 3)]
    param_gamma = [10**i for i in range(-11, 4)]
    param_coef = [10**i for i in range(-4, 4)]
    max_iter = [1000000]
    tol = [1e-3]

    param_grid = {'C': param_c, 'gamma': param_gamma, 'coef0': param_coef, 'max_iter': max_iter, 'tol': tol}
    grid_search = GridSearchCV(SVC(kernel="linear"), param_grid, cv=cv)
    grid_search.fit(X, y)
    return grid_search.best_params_, grid_search.best_score_, grid_search.cv_results_


def bagging_param_selection(X, y, cv, classifier):

    """
    The method aims to find best parameters for bagging learning model.
    To accomplish this, It uses Grid Search Cross Validation method.
    For base model of bagging classifier, svc learning model with best parameters is chosen.

    A set of values are determined for each parameter of bagging learning model.
    Grid Search chooses one of those values which maximizes the classification accuracy.

    Parameters
    ----------
    X: features of training data
    y: labels of training data
    cv: cross validation object which will be applied during parameter search operation
    classifier: base classifier for bagging learning model

    """

    max_samples = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]
    max_features = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

    param_grid = {'max_samples': max_samples, 'max_features': max_features}
    grid_search = GridSearchCV(BaggingClassifier(classifier), param_grid, cv=cv)
    grid_search.fit(X, y)
    return grid_search.best_params_, grid_search.best_score_, grid_search.cv_results_


def write_output(predictions):

    with open('submission.csv', mode='w') as output_file:

        output_writer = csv.writer(output_file, delimiter=',')
        output_writer.writerow(["ID", "Predicted"])

        for i in range(1, len(predictions) + 1):
            output_writer.writerow([i, int(predictions[i - 1])])


# ********** MAIN PROGRAM ********** #


train_x, train_y, test_x = load_data()
new_train_x, new_train_y = outlier_detection(train_x, train_y)
reduced_train_x, reduced_test_x = feature_selection(new_train_x, new_train_y, test_x)


# Best parameters are determined for support vector classifier (svc).
# Then, svc learning model is created with those best parameters.

print("SVC")
best_params, best_score, cv_results = svc_param_selection(reduced_train_x, new_train_y,
                                                          ShuffleSplit(n_splits=10, test_size=0.25, random_state=0))
print("Best Params: ", best_params)
print("Best Score: ", best_score)
print("Mean Test Score: ", mean(cv_results["mean_test_score"]))
best_svc = SVC(kernel="linear", C=best_params["C"], gamma=best_params["gamma"], random_state=5)


# Best parameters are determined for bagging classifier.
# When determining those parameters, best svc is chosen as base model of bagging classifier.

print("\nBagging + SVC")
best_params, best_score, cv_results = bagging_param_selection(reduced_train_x, new_train_y,
                                                              ShuffleSplit(n_splits=10, test_size=0.25, random_state=0),
                                                              best_svc)
print("Best Params: ", best_params)
print("Best Score: ", best_score)
print("Mean Test Score: ", mean(cv_results["mean_test_score"]))

# Bagging classifier is created by using best parameters of svc and bagging

clf = BaggingClassifier(best_svc, max_samples=best_params["max_samples"], max_features=best_params["max_features"],
                        random_state=5)
clf.fit(reduced_train_x, new_train_y)
predictions = clf.predict(reduced_test_x)
write_output(predictions)
