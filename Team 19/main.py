"""
Target Problem:
---------------
* A classifier for the diagnosis of Autism Spectrum Disorder (ASD)

Proposed Solution (Machine Learning Pipeline):
----------------------------------------------
* PCA -> KNN

Input to Proposed Solution:
---------------------------
* Directories of training and testing data in csv file format
* These two types of data should be stored in n x m pattern in csv file format.

  Typical Example:
  ----------------
  n x m samples in training csv file (n number of samples, m - 1 number of features, ground truth labels at last column)
  k x s samples in testing csv file (k number of samples, s number of features)

* These data set files are ready by load_data() function.
* For comprehensive information about input format, please check the section
  "Data Sets and Usage Format of Source Codes" in README.md file on github.

Output of Proposed Solution:
----------------------------
* Predictions generated by learning model for testing set
* They are stored in "submission.csv" file.

Code Owner:
-----------
* Copyright © Team 19. All rights reserved.
* Copyright © Istanbul Technical University, Learning From Data Spring 2019. All rights reserved. """

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler


def load_data(train_path, test_path):

    """
    The train and test data are read from their source files in data-frame format.
    Then, they are converted into numpy array and returned.

    Parameters
    ----------
    train_path: directory of train data set file
    test_path: directory of test data set file
    """

    train_data = np.asarray(pd.read_csv(train_path, skiprows=0))
    test_data = np.asarray(pd.read_csv(test_path, skiprows=0))
    return train_data, test_data


def preprocessing(train_x, test_x):

    """
    The method synthesizes new 70 features by using pca.
    In this way, the dimension of train and test data is reduced.

    Parameters
    ----------
    train_x: features of train data
    test_x: features of test data

    """

    pca = PCA(n_components=70)
    x_train_clean = pca.fit_transform(train_x)
    x_test_clean = pca.transform(test_x)
    return x_train_clean, x_test_clean


def find_component(train_x):

    """
    This method is used to determine the number of features that data samples are reduced to.

    Parameters
    ----------
    train_x: features of train data
    """

    scaler = MinMaxScaler(feature_range=[0, 1])
    data_rescaled = scaler.fit_transform(train_x)
    pca = PCA().fit(data_rescaled)

    plt.Figure()
    plt.plot(np.cumsum(pca.explained_variance_ratio_))
    plt.xlabel('Number of components')
    plt.ylabel('Variance')
    plt.show()


def train_model(x_train, y_train):

    """
    The method creates KNN learning model and trains it by using training data.
    It returns trained learning model.

    Parameters
    ----------
    x_train: features of training data
    y_train: labels of training data

    """

    classifier = KNeighborsClassifier(n_neighbors=5)
    classifier.fit(x_train, y_train)
    return classifier


def predict(model, x_test):

    """
    The method predicts labels for testing data samples by using trained learning model.

    Parameters
    ----------
    model: trained learning model (KNN)
    x_test: features of testing data

    """

    predictions = model.predict(x_test)
    predictions.shape = (np.size(predictions), 1)
    return predictions


def write_output(predictions):

    temp = np.ones((80, 1), dtype=float)
    for i in range(0, 80):
        temp[i] = i + 1

    y_csv = np.concatenate((temp, predictions), 1)
    np.savetxt('submission.csv', y_csv, delimiter=",", comments='', fmt='%.0f', header="ID,Predicted")


# ********** MAIN PROGRAM ********** #

train_data, test_data = load_data("train.csv", "test.csv")
x_train = train_data[:, :595]
y_train = train_data[:, 595]

# find_component(x_train) -> it shows why 70 components are needed for pca

x_train_clean, x_test_clean = preprocessing(x_train, test_data)
classifier = train_model(x_train_clean, y_train)
predictions = predict(classifier, x_test_clean)
write_output(predictions)
