"""
Target Problem:
---------------
* A classifier for the diagnosis of Autism Spectrum Disorder (ASD)

Proposed Solution (Machine Learning Pipeline):
----------------------------------------------
* Standard Scaling -> PCA -> Voting Classifier

Input to Proposed Solution:
---------------------------
* Directories of training and testing data in csv file format
* These two types of data should be stored in n x m pattern in csv file format.

  Typical Example:
  ----------------
  n x m samples in training csv file (n number of samples, m - 1 number of features, ground truth labels at last column)
  k x s samples in testing csv file (k number of samples, s number of features)

* These data set files are ready by load_data() function.
* For comprehensive information about input format, please check the section
  "Data Sets and Usage Format of Source Codes" in README.md file on github.

Output of Proposed Solution:
----------------------------
* Predictions generated by learning model for testing set
* They are stored in "submission.csv" file.

Code Owner:
-----------
* Copyright © Team 3. All rights reserved.
* Copyright © Istanbul Technical University, Learning From Data Spring 2019. All rights reserved. """

import csv
import warnings

from classifiers import *
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score


def load_data(tra_file, tst_file):

    """
    This method reads the files in which training and testing data samples are located.
    Then, it returns training and testing data set in numpy array format.

    Parameters
    ----------
    tra_file: directory name of training dataset file
    tst_file: directory name of testing dataset file
    """

    x_tra = np.genfromtxt(tra_file, delimiter=',')
    x_tst = np.genfromtxt(tst_file, delimiter=',')

    # delete first rows
    x_tra = np.delete(x_tra, 0, 0)
    x_tst = np.delete(x_tst, 0, 0)
    y_tra = x_tra[:, -1]

    # delete class row
    x_tra = np.delete(x_tra, -1, 1)
    return x_tra, x_tst, y_tra


def preprocessing(x_tra, x_tst):

    """
    Training and testing data set are at first scaled by using standard scaler method.
    Then, these data sets are reduced to lower dimension by using feature extraction pca technique.
    When performing these two operations, testing data set is not included in fit operation of scaler and pca.

    Parameters
    ----------
    x_tra: training data set (they should not contain label values)
    x_tst: testing data set
    :return: reduced and scaled training and testing sets
    """

    scaler = StandardScaler()
    scaler.fit(x_tra)

    x_tra_scaled = scaler.transform(x_tra)
    x_tst_scaled = scaler.transform(x_tst)

    pca = PCA(.95)
    pca.fit(x_tra_scaled)
    x_tra_reduced = pca.transform(x_tra_scaled)
    x_tst_reduced = pca.transform(x_tst_scaled)

    return x_tra_reduced, x_tst_reduced


def train_model(x_tra_reduced, y_tra, model):

    model.fit(x_tra_reduced, y_tra)
    return model


def predict(model, x_tst_reduced):

    predictions = model.predict(x_tst_reduced)
    return predictions


def write_output(prediction, filename):

    with open(filename, 'w', newline='') as csvfile:
        filewriter = csv.writer(csvfile, delimiter=',')
        filewriter.writerow(["ID", "Predicted"])
        id = 1

        for row in prediction:
            filewriter.writerow([id, row.astype(int)])
            id += 1


# RandomForestClassifier gives lots of warnings
# therefore this line is added below
warnings.filterwarnings("ignore")

# Load data
Xtra, Xtst, Ytra = load_data('train.csv', 'test.csv')
Xtra_reduced, Xtst_reduced = preprocessing(Xtra, Xtst)


# *************** SECTION 1 *************** #

# Each of models would be trained and their cross validation score would be printed in this section.

print("Classifiers cross-validation")

labels_clf = ['RandomForest', 'ExtraTrees', 'KNeighbors', 'SVC', 'Ridge', 'LogisticRegression', 'GaussianNB',
              'DecisionTree']

for model, label in zip([rf, et, knn, svc, rg, lr, gnb, dt], labels_clf):

    scores = cross_val_score(model, Xtra_reduced, Ytra, cv=5, scoring='accuracy')
    trained_model = train_model(Xtra_reduced, Ytra, model)
    prediction = predict(trained_model, Xtst_reduced)

    print("Mean: {0:.3f}, std: {1:.3f} [{2} is used.]".format(scores.mean(), scores.std(), label))

print("-----------------------------------\n")

# *************** SECTION 2 *************** #

# Ensemble models would be trained and their cross validation scores would be printed in this section

print("Bagging, Boosting and GridSearchCV cross-validation")

labels = ['Ada Boost', 'Ada BoostSVC', 'Grad Boost', 'XG Boost', 'Ensemble', 'Voting',
          'BaggingWithRF', 'Grid']

for model, label in zip([ada_boost, ada_boost_svc, grad_boost, xgb_boost, ev_clf, vclf, bagging_clf, grid], labels):

    scores = cross_val_score(model, Xtra_reduced, Ytra, cv=5, scoring='accuracy')
    trained_model = train_model(Xtra_reduced, Ytra, model)
    prediction = predict(trained_model, Xtst_reduced)

    if label == "Ensemble":
        write_output(prediction, label + ".csv")

    if label == "Ensemble":
        print("Mean: {0:.3f}, std: {1:.3f} [*{2} is used. (Chosen model)]".format(scores.mean(), scores.std(), label))

    else:
        print("Mean: {0:.3f}, std: {1:.3f} [{2} is used.]".format(scores.mean(), scores.std(), label))
